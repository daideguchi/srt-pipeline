#!/usr/bin/env python3
"""
SRT Utility Functions - ã‚·ãƒ³ãƒ—ãƒ«ãªå­—å¹•è§£æãƒ„ãƒ¼ãƒ«
"""

import re
import unicodedata
from typing import List, Dict, Any, Tuple
from dataclasses import dataclass


@dataclass
class SRTSegment:
    index: int
    start: float
    end: float
    text: str


def parse_timestamp(timestamp: str) -> float:
    """SRTã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ç§’ã«å¤‰æ›"""
    # 00:00:05,860 -> 5.860
    parts = timestamp.split(':')
    hours = int(parts[0])
    minutes = int(parts[1])
    seconds_parts = parts[2].split(',')
    seconds = int(seconds_parts[0])
    milliseconds = int(seconds_parts[1])
    
    total_seconds = hours * 3600 + minutes * 60 + seconds + milliseconds / 1000
    return total_seconds


def load_srt(file_path: str) -> List[SRTSegment]:
    """SRTãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
    segments = []
    
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã”ã¨ã«åˆ†å‰²
    segment_blocks = re.split(r'\n\s*\n', content.strip())
    
    for block in segment_blocks:
        lines = block.strip().split('\n')
        if len(lines) >= 3:
            try:
                index = int(lines[0])
                timeline = lines[1]
                text = '\n'.join(lines[2:])
                
                # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—è§£æ
                start_str, end_str = timeline.split(' --> ')
                start = parse_timestamp(start_str)
                end = parse_timestamp(end_str)
                
                segments.append(SRTSegment(
                    index=index,
                    start=start,
                    end=end,
                    text=text
                ))
            except (ValueError, IndexError):
                continue
    
    return segments


def analyze_srt_quality(segments: List[SRTSegment]) -> Dict[str, Any]:
    """SRTå“è³ªåˆ†æ"""
    if not segments:
        return {"error": "No segments found"}
    
    # åŸºæœ¬çµ±è¨ˆ
    total_duration = segments[-1].end if segments else 0
    durations = [seg.end - seg.start for seg in segments]
    
    # CPSï¼ˆæ–‡å­—/ç§’ï¼‰è¨ˆç®—
    cps_values = []
    for seg in segments:
        text_len = len(unicodedata.normalize("NFKC", seg.text.replace('\n', '').replace(' ', '')))
        duration = seg.end - seg.start
        if duration > 0:
            cps = text_len / duration
            cps_values.append(cps)
    
    # èª­ã¿ã‚„ã™ã•åˆ†æ
    short_segments = sum(1 for d in durations if d < 1.0)
    long_segments = sum(1 for d in durations if d > 10.0)
    high_cps = sum(1 for cps in cps_values if cps > 6.0)
    
    # è¡Œæ•°åˆ†æ
    line_counts = [seg.text.count('\n') + 1 for seg in segments]
    long_lines = sum(1 for seg in segments for line in seg.text.split('\n') if len(line) > 15)
    
    analysis = {
        "basic_stats": {
            "total_segments": len(segments),
            "total_duration": round(total_duration, 1),
            "avg_duration": round(sum(durations) / len(durations), 2),
            "min_duration": round(min(durations), 2),
            "max_duration": round(max(durations), 2)
        },
        "readability": {
            "avg_cps": round(sum(cps_values) / len(cps_values), 2) if cps_values else 0,
            "max_cps": round(max(cps_values), 2) if cps_values else 0,
            "high_cps_count": high_cps,
            "avg_lines": round(sum(line_counts) / len(line_counts), 1),
            "long_lines": long_lines
        },
        "timing_issues": {
            "short_segments": short_segments,
            "long_segments": long_segments,
            "gaps": count_gaps(segments),
            "overlaps": count_overlaps(segments)
        },
        "content_analysis": {
            "total_chars": sum(len(seg.text.replace('\n', '').replace(' ', '')) for seg in segments),
            "punctuation_endings": count_punctuation_endings(segments)
        }
    }
    
    return analysis


def count_gaps(segments: List[SRTSegment]) -> int:
    """ã‚»ã‚°ãƒ¡ãƒ³ãƒˆé–“ã®ç©ºç™½ã‚’æ•°ãˆã‚‹"""
    gaps = 0
    for i in range(len(segments) - 1):
        if segments[i + 1].start > segments[i].end + 0.1:  # 0.1ç§’ä»¥ä¸Šã®ç©ºç™½
            gaps += 1
    return gaps


def count_overlaps(segments: List[SRTSegment]) -> int:
    """é‡è¤‡ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’æ•°ãˆã‚‹"""
    overlaps = 0
    for i in range(len(segments) - 1):
        if segments[i].end > segments[i + 1].start:
            overlaps += 1
    return overlaps


def count_punctuation_endings(segments: List[SRTSegment]) -> int:
    """å¥èª­ç‚¹ã§çµ‚ã‚ã‚‹ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã‚’æ•°ãˆã‚‹"""
    endings = "ã€‚ï¼ï¼Ÿâ€¦ã€ã€"
    count = 0
    for seg in segments:
        if seg.text.strip() and seg.text.strip()[-1] in endings:
            count += 1
    return count


def compare_with_script(segments: List[SRTSegment], script_path: str) -> Dict[str, Any]:
    """å°æœ¬ã¨ã®æ¯”è¼ƒåˆ†æ"""
    try:
        with open(script_path, 'r', encoding='utf-8') as f:
            script_content = f.read()
        
        # ãƒ†ã‚­ã‚¹ãƒˆæ­£è¦åŒ–
        def normalize_text(text):
            text = unicodedata.normalize("NFKC", text)
            text = re.sub(r'[ã€Œã€ã€ã€\s\n]', '', text)  # æ‹¬å¼§ã¨ç©ºç™½å‰Šé™¤
            return text
        
        script_normalized = normalize_text(script_content)
        srt_text = ''.join(seg.text for seg in segments)
        srt_normalized = normalize_text(srt_text)
        
        # é¡ä¼¼åº¦è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        from difflib import SequenceMatcher
        similarity = SequenceMatcher(None, script_normalized, srt_normalized).ratio()
        
        return {
            "script_chars": len(script_normalized),
            "srt_chars": len(srt_normalized),
            "similarity": round(similarity, 3),
            "coverage": round(len(srt_normalized) / len(script_normalized), 3) if script_normalized else 0
        }
    
    except Exception as e:
        return {"error": str(e)}


def generate_quality_report(srt_path: str, script_path: str = None) -> str:
    """å“è³ªãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
    segments = load_srt(srt_path)
    analysis = analyze_srt_quality(segments)
    
    report = f"""ğŸ” SRTå­—å¹•å“è³ªãƒ¬ãƒãƒ¼ãƒˆ
{'='*50}

ğŸ“Š åŸºæœ¬çµ±è¨ˆ:
- ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°: {analysis['basic_stats']['total_segments']}
- ç·æ™‚é–“: {analysis['basic_stats']['total_duration']}ç§’
- å¹³å‡ã‚»ã‚°ãƒ¡ãƒ³ãƒˆé•·: {analysis['basic_stats']['avg_duration']}ç§’
- æœ€çŸ­/æœ€é•·: {analysis['basic_stats']['min_duration']}/{analysis['basic_stats']['max_duration']}ç§’

ğŸ“– èª­ã¿ã‚„ã™ã•:
- å¹³å‡CPS: {analysis['readability']['avg_cps']} (ç†æƒ³: 4-6)
- æœ€å¤§CPS: {analysis['readability']['max_cps']}
- é«˜CPS(>6)ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ: {analysis['readability']['high_cps_count']}å€‹
- å¹³å‡è¡Œæ•°: {analysis['readability']['avg_lines']}
- é•·ã„è¡Œ(>15å­—): {analysis['readability']['long_lines']}å€‹

â° ã‚¿ã‚¤ãƒŸãƒ³ã‚°å“è³ª:
- çŸ­ã™ã(<1s): {analysis['timing_issues']['short_segments']}å€‹
- é•·ã™ã(>10s): {analysis['timing_issues']['long_segments']}å€‹
- ç©ºç™½: {analysis['timing_issues']['gaps']}ç®‡æ‰€
- é‡è¤‡: {analysis['timing_issues']['overlaps']}ç®‡æ‰€

ğŸ“ å†…å®¹å“è³ª:
- ç·æ–‡å­—æ•°: {analysis['content_analysis']['total_chars']}
- å¥èª­ç‚¹çµ‚äº†: {analysis['content_analysis']['punctuation_endings']}å€‹
"""
    
    # å°æœ¬æ¯”è¼ƒãŒã‚ã‚Œã°è¿½åŠ 
    if script_path:
        comparison = compare_with_script(segments, script_path)
        if 'error' not in comparison:
            report += f"""
ğŸ“‹ å°æœ¬æ¯”è¼ƒ:
- å°æœ¬æ–‡å­—æ•°: {comparison['script_chars']}
- å­—å¹•æ–‡å­—æ•°: {comparison['srt_chars']}
- é¡ä¼¼åº¦: {comparison['similarity']} (1.0ãŒå®Œå…¨ä¸€è‡´)
- ã‚«ãƒãƒ¼ç‡: {comparison['coverage']} (1.0ãŒå®Œå…¨ã‚«ãƒãƒ¼)
"""
    
    # å“è³ªè©•ä¾¡
    issues = []
    if analysis['readability']['avg_cps'] > 6:
        issues.append("âš ï¸ CPSé«˜ã™ã (èª­ã¿ã«ãã„å¯èƒ½æ€§)")
    if analysis['timing_issues']['short_segments'] > 5:
        issues.append("âš ï¸ çŸ­ã„ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãŒå¤šã„")
    if analysis['readability']['long_lines'] > 10:
        issues.append("âš ï¸ é•·ã„è¡ŒãŒå¤šã„")
    
    if issues:
        report += f"\nğŸš¨ æ¤œå‡ºã•ã‚ŒãŸå•é¡Œ:\n" + "\n".join(f"- {issue}" for issue in issues)
    else:
        report += f"\nâœ… å“è³ªè‰¯å¥½ï¼"
    
    return report


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    srt_file = "subs/final_production_direct.srt"
    script_file = "script_4_2.txt"
    
    report = generate_quality_report(srt_file, script_file)
    print(report)