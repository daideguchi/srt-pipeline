#!/usr/bin/env python3
"""
smart_subs.py - 4ã‚½ãƒ¼ã‚¹çµ±åˆã§é«˜å“è³ªãªæ—¥æœ¬èªSRTã‚’ç”Ÿæˆ
å‰æ: æ—¢å­˜JSONã‚’å¿…ãšä½¿ç”¨ï¼ˆå†è§£æç¦æ­¢ï¼‰
- audio_json: ç„¡éŸ³valleyå«ã‚€æ—¢å­˜è§£æï¼ˆffmpeg silencedetectç”±æ¥ã®æ•´å½¢JSONï¼‰
- whisper_json: whisper-cppå½¢å¼ã®æ—¢å­˜ASR JSON
- capcut_srt: CapCut/Vrewã®ç²—ã„SRTï¼ˆæ™‚é–“æ ã¯æœ‰ç”¨ï¼‰
- script: å°æœ¬ãƒ†ã‚­ã‚¹ãƒˆï¼ˆå†…å®¹ã®æ­£ï¼‰
"""

import json, re, unicodedata
from pathlib import Path
from typing import List, Tuple, Dict, Any, Optional
from srt_rules import norm_text, enforce_layout, MAX_LINE_CHARS_DEFAULT, MAX_LINES_DEFAULT, MAX_CPS_DEFAULT

MIN_DUR = 1.2
MIN_GAP = 0.12
VALLEY_SNAP_MS = 150  # Â±150ms
ANCHOR_PATTERNS = [
    "æœ€å¾Œã¾ã§ã”è¦–è´ã„ãŸã ãã€æœ¬å½“ã«ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸ",
    "æœ€å¾Œã¾ã§ã”è¦–è´ã„ãŸã ãæœ¬å½“ã«ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸ",
    "ã”è¦–è´ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã—ãŸ",
]

def fmt_time(sec: float) -> str:
    sec = max(0.0, sec)
    h = int(sec // 3600)
    m = int((sec % 3600) // 60)
    s = int(sec % 60)
    ms = int(round((sec - int(sec)) * 1000))
    return f"{h:02d}:{m:02d}:{s:02d},{ms:03d}"

def parse_srt(path: str) -> List[Dict[str, Any]]:
    text = Path(path).read_text(encoding="utf-8", errors="ignore")
    blocks = re.split(r"\n\s*\n", text.strip())
    cues = []
    for b in blocks:
        lines = [ln for ln in b.splitlines() if ln.strip()]
        if not lines:
            continue
        if re.fullmatch(r"\d+", lines[0]):
            lines = lines[1:]
        if not lines:
            continue
        m = re.match(r"(\d{2}:\d{2}:\d{2},\d{3})\s*-->\s*(\d{2}:\d{2}:\d{2},\d{3})", lines[0])
        if not m:
            continue

        def tosec(ts: str) -> float:
            h, m, sms = ts.split(":")
            s, ms = sms.split(",")
            return int(h)*3600 + int(m)*60 + int(s) + int(ms)/1000.0

        st, ed = tosec(m.group(1)), tosec(m.group(2))
        txt = "\n".join(lines[1:])
        cues.append({"start": st, "end": ed, "text": txt})
    return cues

def format_srt(cues: List[Dict[str, Any]]) -> str:
    out = []
    for i, c in enumerate(cues, 1):
        out.append(str(i))
        out.append(f"{fmt_time(c['start'])} --> {fmt_time(c['end'])}")
        out.append(c.get("text", "").rstrip())
        out.append("")
    return "\n".join(out).strip() + "\n"

def format_vtt(cues: List[Dict[str, Any]]) -> str:
    lines = ["WEBVTT", ""]
    for c in cues:
        st = fmt_time(c["start"]).replace(",", ".")
        ed = fmt_time(c["end"]).replace(",", ".")
        lines.append(f"{st} --> {ed}")
        lines.append(c.get("text","").rstrip())
        lines.append("")
    return "\n".join(lines).strip() + "\n"

def load_whisper_chars(wh_json: str):
    """
    whisper-cpp JSONã«å¯¾å¿œ:
    {
      "transcription": [
        {
          "text": "...",
          "offsets": {"from": ms, "to": ms},
          "tokens": [{"text":"â€¦", "offsets":{"from":ms,"to":ms}}, ...]
        }, ...
      ]
    }
    æˆ»ã‚Šå€¤: (asr_text, char_times, est_audio_dur)
    """
    try:
        data = json.loads(Path(wh_json).read_text(encoding="utf-8"))
    except UnicodeDecodeError:
        data = json.loads(Path(wh_json).read_text(encoding="utf-8", errors="replace"))

    asr_text = ""
    char_times: List[float] = []
    duration = None

    if "transcription" in data:
        trs = data["transcription"]
        for seg in trs:
            seg_start = seg.get("offsets", {}).get("from", 0) / 1000.0
            seg_end   = seg.get("offsets", {}).get("to", 0) / 1000.0
            seg_text  = norm_text(seg.get("text",""))
            tokens = seg.get("tokens") or []
            if tokens:
                for tk in tokens:
                    t_text = norm_text(tk.get("text",""))
                    if not t_text:
                        continue
                    t_start = tk.get("offsets", {}).get("from", seg_start*1000)/1000.0
                    t_end   = tk.get("offsets", {}).get("to", seg_end*1000)/1000.0
                    dur = max(0.001, t_end - t_start)
                    step = dur / max(1, len(t_text))
                    for i, ch in enumerate(t_text):
                        asr_text += ch
                        char_times.append(t_start + step * i)
            else:
                if seg_text:
                    dur = max(0.001, seg_end - seg_start)
                    step = dur / max(1, len(seg_text))
                    for i, ch in enumerate(seg_text):
                        asr_text += ch
                        char_times.append(seg_start + step * i)
        if trs:
            duration = trs[-1].get("offsets", {}).get("to", 0)/1000.0

    elif "segments" in data:  # å¾Œæ–¹äº’æ›ï¼ˆfaster-whisperï¼‰
        for seg in data["segments"]:
            for w in seg.get("words", []) or []:
                word = norm_text(w.get("text",""))
                if not word:
                    continue
                wst, wed = w.get("start", 0.0), w.get("end", 0.0)
                dur = max(0.001, wed - wst)
                step = dur / max(1, len(word))
                for i, ch in enumerate(word):
                    asr_text += ch
                    char_times.append(wst + step * i)
        duration = data.get("duration")

    return asr_text, char_times, duration

def load_valleys(audio_json: str):
    """
    æ—¢å­˜ã®ç„¡éŸ³è§£æJSONã‚’èª­ã¿ã€valleyé…åˆ—ã¨æ¨å®šaudioé•·ã‚’è¿”ã™
    æœŸå¾…æ§‹é€ :
    {
      "duration": 517.331,
      "silences": [{"start":..,"end":..,"duration":..,"valley":..}, ...]
    }
    """
    data = json.loads(Path(audio_json).read_text(encoding="utf-8"))
    silences = data.get("silences", [])
    valleys = [s["valley"] for s in silences if "valley" in s]
    audio_len = data.get("duration")
    return valleys, audio_len

def nearest_valley(valleys: List[float], t: float, radius_ms: int) -> float:
    if not valleys:
        return t
    radius = max(0.0, radius_ms) / 1000.0
    best, bd = t, 1e9
    for v in valleys:
        d = abs(v - t)
        if d < bd:
            bd = d; best = v
    return best if bd <= radius else t

def cut_tail_at_anchor(cues: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    # ã‚¢ãƒ³ã‚«ãƒ¼æ–‡ã‚’å«ã‚€æœ€å¾Œã®ã‚­ãƒ¥ãƒ¼ä»¥é™ã‚’æ¨ã¦ã‚‹
    def has_anchor(txt: str) -> bool:
        T = re.sub(r"\s+", "", txt or "")
        return any(p in T for p in ANCHOR_PATTERNS)
    for i in range(len(cues)-1, -1, -1):
        if has_anchor(cues[i]["text"]):
            return cues[:i+1]
    return cues

def distribute_text_over_capcut(script: str, capcut_cues: List[Dict[str,Any]], valleys: List[float]):
    """
    CapCutã®æ ã‚’ä½¿ã„ã€å°æœ¬æ–‡å­—åˆ—ã‚’è‡ªç„¶ã«é…åˆ†ã€‚
    - æ è¨±å®¹é‡: duration * MAX_CPS_DEFAULT
    - æœ«å°¾ã¯å¥èª­ç‚¹å„ªå…ˆï¼ˆå¼· > å¼± > å®¹é‡ï¼‰
    - æ é–‹å§‹/çµ‚äº†ã¯ valley ã«Â±150msã§ã‚¹ãƒŠãƒƒãƒ—ï¼ˆé€†è»¢ã—ãã†ãªã‚‰å…ƒã«æˆ»ã™ï¼‰
    - çŸ­ã™ãã‚‹æ ã¯å‰æ–¹ãƒãƒ¼ã‚¸ã€ã‚®ãƒ£ãƒƒãƒ—ä¸è¶³ã¯ç­‰åˆ†èª¿æ•´
    - æœ€çµ‚çš„ã«2è¡Œ/36å­—ãƒ»CPS<=17ã®æ•´å½¢ã‚’é©ç”¨
    """
    S = re.sub(r"\s+", " ", norm_text(script)).strip()
    N = len(S)
    tptr = 0
    out: List[Dict[str,Any]] = []

    for c in capcut_cues:
        st, ed = float(c["start"]), float(c["end"])
        dur = max(0.001, ed - st)
        quota = int(dur * MAX_CPS_DEFAULT)

        if tptr >= N:
            txt = ""
        else:
            end_guess = min(N, tptr + max(1, quota))
            chunk = S[tptr:end_guess]

            # å¼·ã„å¥èª­ç‚¹ã‚’æœ«å°¾ã«ï¼ˆç›´è¿‘æœ€å¾Œï¼‰
            m = re.search(r"[ã€‚ï¼ï¼Ÿ](?!.*[ã€‚ï¼ï¼Ÿ])", chunk)
            if m:
                cut = tptr + m.end()
            else:
                # å¼±ã„å¥èª­ç‚¹
                m2 = re.search(r"[ã€ï¼Œ](?!.*[ã€ï¼Œ])", chunk)
                # â˜…ãƒã‚°ä¿®æ­£æ¸ˆ: tpträºŒé‡åŠ ç®—ã—ãªã„
                cut = tptr + (m2.end() if m2 else len(chunk))

            cut = max(cut, tptr+1)
            cut = min(cut, N)
            txt = S[tptr:cut]
            tptr = cut

        st2 = nearest_valley(valleys, st, VALLEY_SNAP_MS) if valleys else st
        ed2 = nearest_valley(valleys, ed, VALLEY_SNAP_MS) if valleys else ed
        if ed2 - st2 < 0.4:
            st2, ed2 = st, ed

        out.append({"start": st2, "end": ed2, "text": txt})

    # æ®‹æ–‡ãŒã‚ã‚Œã°æœ«å°¾ã¸å¸å
    if tptr < N and out:
        out[-1]["text"] = (out[-1]["text"] + S[tptr:]).strip()
        tptr = N

    # ç©ºãƒ†ã‚­ã‚¹ãƒˆã®ä¸­é–“ã‚­ãƒ¥ãƒ¼ã¯å‰ã¸çµ±åˆ
    i = 0
    while i < len(out)-1:
        if not (out[i]["text"] or "").strip():
            if i > 0:
                out[i-1]["end"] = out[i]["end"]
            out.pop(i)
        else:
            i += 1

    # çŸ­å°ºãƒãƒ¼ã‚¸ & ã‚®ãƒ£ãƒƒãƒ—èª¿æ•´
    j = 0
    while j < len(out):
        cur = out[j]
        dur = cur["end"] - cur["start"]
        if dur < MIN_DUR and j > 0:
            out[j-1]["end"] = cur["end"]
            out[j-1]["text"] = (out[j-1]["text"] + cur["text"]).strip()
            out.pop(j); continue
        if j > 0:
            gap = cur["start"] - out[j-1]["end"]
            if gap < MIN_GAP:
                shift = (MIN_GAP - gap) / 2
                out[j-1]["end"] -= shift
                cur["start"] += shift
        j += 1

    # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆæ•´å½¢
    for c in out:
        dur = max(0.001, c["end"] - c["start"])
        lines, _ok = enforce_layout(c["text"], dur,
                                    max_chars_per_line=MAX_LINE_CHARS_DEFAULT,
                                    max_lines=MAX_LINES_DEFAULT,
                                    max_cps=MAX_CPS_DEFAULT)
        c["text"] = "\n".join(lines)

    return out

def main():
    import argparse
    ap = argparse.ArgumentParser(description="4ã‚½ãƒ¼ã‚¹çµ±åˆï¼ˆæ—¢å­˜JSONã®ã¿ä½¿ç”¨ï¼‰")
    ap.add_argument("--audio_json", required=True, help="æ—¢å­˜ã®ç„¡éŸ³è§£æJSONï¼ˆvalleyå«ã‚€ï¼‰")
    ap.add_argument("--whisper_json", required=True, help="æ—¢å­˜ã®ASR JSONï¼ˆwhisper-cppå½¢å¼ï¼‰")
    ap.add_argument("--capcut_srt", required=True, help="CapCut/Vrewã®SRT")
    ap.add_argument("--script", required=True, help="å°æœ¬ãƒ†ã‚­ã‚¹ãƒˆ")
    ap.add_argument("--out", required=True, help="å‡ºåŠ›SRT")
    ap.add_argument("--out_vtt", required=True, help="å‡ºåŠ›VTT")
    args = ap.parse_args()

    valleys, audio_len = load_valleys(args.audio_json)
    capcut = parse_srt(args.capcut_srt)
    script = Path(args.script).read_text(encoding="utf-8")
    # è§£æJSONã®å¥å…¨æ€§ç¢ºèªï¼ˆâ€»å°†æ¥ã®å¢ƒç•Œç²¾åº¦èª¿æ•´ç”¨ã«ãƒ­ãƒ¼ãƒ‰ã®ã¿ï¼‰
    _asr_text, _char_times, _est_dur = load_whisper_chars(args.whisper_json)

    cues = distribute_text_over_capcut(script, capcut, valleys)
    cues = cut_tail_at_anchor(cues)

    # æ™‚åˆ»ã®å¥å…¨æ€§
    for c in cues:
        if c["end"] <= c["start"]:
            c["end"] = c["start"] + 1.2
    cues = sorted(cues, key=lambda x: x["start"])

    Path(args.out).parent.mkdir(parents=True, exist_ok=True)
    Path(args.out).write_text(format_srt(cues), encoding="utf-8")
    Path(args.out_vtt).write_text(format_vtt(cues), encoding="utf-8")

    last_end = cues[-1]["end"] if cues else 0.0
    print("ğŸ”„ 4ã‚½ãƒ¼ã‚¹çµ±åˆå‡¦ç†å®Œäº†")
    print(f"  CapCut SRT: {len(capcut)}")
    print(f"  å‡ºåŠ›: {len(cues)}  æœ€çµ‚çµ‚äº†: {last_end:.3f}s  (éŸ³å£°é•· ~{(audio_len or 0):.3f}s)")

if __name__ == "__main__":
    main()